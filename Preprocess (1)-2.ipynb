{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfc94c1",
   "metadata": {},
   "source": [
    "# <center><u>Données massives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61dffc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6575111f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\User_K\\\\OneDrive\\\\Desktop\\\\credit_risk_dataset.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8r/65mjgr2n0vdd_btq1mxbstyh0000gn/T/ipykernel_24292/3758422778.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mData\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"C:\\Users\\User_K\\OneDrive\\Desktop\\credit_risk_dataset.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m                 )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m     ) as handle:\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\User_K\\\\OneDrive\\\\Desktop\\\\credit_risk_dataset.xlsx'"
     ]
    }
   ],
   "source": [
    "Data=pd.read_excel(r\"C:\\Users\\User_K\\OneDrive\\Desktop\\credit_risk_dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79403760",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53186d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8abcdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rate=Data.isna().sum()/Data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c92de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460c246",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.heatmap(Data.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1bc56f",
   "metadata": {},
   "source": [
    "On remarque qu'on a des données manquants au niveau de la variable \"person_emp_length\" qui refere aux nombres d'années d'emploie avec un pourcentage de 3% à peu pres ainsi que 9.5% à peu pres des données de la variable \"loan_int_rate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ead541",
   "metadata": {},
   "source": [
    "-Puisque on a une grande base de données et que le pourcentage des données manquants au niveau de la variable \"person_emp_length\" on va supprimer ces données manquants ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1986b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars=['Index','loan_int_rate']\n",
    "dt1= pd.DataFrame(data=Data,columns=vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82fdccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.drop('loan_int_rate',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c641f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=Data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dead0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e29c15",
   "metadata": {},
   "source": [
    "Notre Base est maintenant sans valeurs manquante mais aussi sans la variale 'loan_int_rate'. On a maintenant merger notre colonne et la base en utlisant Index comme cle. Puis on va essayer de traiter les valeurs manquantes au niveau de cette variable qui reste puis on va "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.merge(Data,dt1,on='Index',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c54e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6466eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd022141",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['person_home_ownership']=le.fit_transform(data['person_home_ownership'])\n",
    "data['loan_intent']=le.fit_transform(data['loan_intent'])\n",
    "data['loan_grade']=le.fit_transform(data['loan_grade'])\n",
    "data['cb_person_default_on_file']=le.fit_transform(data['cb_person_default_on_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e0486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257465ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd70020",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.DataFrame(imputer.fit_transform(data),columns = data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4194712",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['loan_int_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64398866",
   "metadata": {},
   "source": [
    "    Maintenant qu'on a plus de valeurs manquantes, on va passer au traitement des valeurs aberantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[data1['person_age']<data1['person_emp_length']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d36cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[data1['person_age']<23]['person_emp_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[data1['person_age']<23]['person_emp_length'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6959e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EOn a pris la mediane parceque dans la liste qu'on a utilisé on a 2 valeurs aberrantes donc on va remplacer avec 4 :\n",
    "data1['person_emp_length'][0] = 4\n",
    "data1['person_emp_length'][209] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[data1['person_emp_length']>40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fa23d",
   "metadata": {},
   "source": [
    "à partir de plus de 40 ans de carrière, peut être moins plausible, mais dans ce cas, plausible car âge de la personne est  78 ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b76a13",
   "metadata": {},
   "source": [
    "### Maintenant que notre base de données est clean, on pourra commencer la construction de notre modèle. mais avant on va checker le desequilibre de notre base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde26046",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = data1[['loan_status']]\n",
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = data1[['person_age','person_income','person_home_ownership','person_emp_length','loan_intent', 'loan_grade', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_default_on_file', 'cb_person_cred_hist_length']]\n",
    "df_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9e571d",
   "metadata": {},
   "source": [
    "avant de faire de l'oversampling ou undersampling, il faut séparer échantillon test et train car ces 2 méthodes  doivent se      faire uniquement sur l'échantillon train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ddb38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec897f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "x_train_ros, y_train_ros= ros.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed5690",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ros['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7c955",
   "metadata": {},
   "source": [
    "l'échantillon maintenant équilibré, on va pouvoir passer aux modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b333b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ros.to_excel(\"x_train_ros.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ce1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ros.to_excel(\"y_train_ros.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecb60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn import model_selection,linear_model, metrics\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, roc_auc_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70389df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_assess(model, name='Default'):\n",
    "    model.fit(x_train_ros, y_train_ros)\n",
    "    preds = model.predict(x_test)\n",
    "    preds_proba = model.predict_proba(x_test)\n",
    "    print(name, '\\n',classification_report(y_test, model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=150)\n",
    "model_assess(knn, name='KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(random_state=42)\n",
    "model_assess(lg, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20774de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=2, min_samples_leaf=1, random_state=42)\n",
    "model_assess(D_tree, 'DecisionTree Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42) \n",
    "model_assess(xgb, 'XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb90d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Support Vector Machine\",\n",
    "    \"Neural Network\",\n",
    "    \"Random Forest\",\n",
    "    \"XGBoost\",\n",
    "    \"KNeighborsClassifier\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LogisticRegression(),\n",
    "    SVC(),\n",
    "    MLPClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    xgboost.XGBRFClassifier(max_depth=5),\n",
    "    KNeighborsClassifier(n_neighbors=8, p=2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy=[]\n",
    "for model, name in zip(models,names):\n",
    "    model.fit(x_train_ros, y_train_ros)\n",
    "    \n",
    "    y_pred = model.predict(x_test)\n",
    "    print('Confusion matrix of ',name)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy score is ',acc_score)\n",
    "    accuracy.append(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8044f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy=pd.DataFrame(zip(names, accuracy),columns =['Model', 'Accuracy']).sort_values(\"Accuracy\",ascending=False)\n",
    "df_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 5))\n",
    "\n",
    "plt.bar(df_accuracy[\"Model\"], df_accuracy[\"Accuracy\"])\n",
    "plt.xlabel(\"Models\",fontsize=16)\n",
    "plt.ylabel(\"Respective Accuracy\",fontsize=16)\n",
    "plt.title(\"Sorted Accuracy of different Models\", fontsize = 20)\n",
    "plt.xticks(fontsize = 11, horizontalalignment = 'right', rotation = 30)\n",
    "plt.yticks(fontsize = 13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(n_estimators=50, max_features=\"auto\", random_state=44)\n",
    "rf_model.fit(x_train_ros, y_train_ros)\n",
    "predictions = rf_model.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4cb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.predict_proba(x_test).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a9615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(\"The importance of feature \"i\" is \"{round(rf_model.feature_importances_[i] * 100, 2)}\"%.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
